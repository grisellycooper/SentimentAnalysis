{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk.data\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>OK, lets start with the best. the building. al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The British 'heritage film' industry is out of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I don't even know where to begin on this one. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49995  OK, lets start with the best. the building. al...          0\n",
       "49996  The British 'heritage film' industry is out of...          0\n",
       "49997  I don't even know where to begin on this one. ...          0\n",
       "49998  Richard Tyler is a little boy who is scared of...          0\n",
       "49999  I waited long to watch this movie. Also becaus...          1"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('shuffled_movie_data.csv')\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    text = [w for w in text.split() if w not in stop]\n",
    "    tokenized = [porter.stem(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    for _ in range(size):\n",
    "        text, label = next(doc_stream)\n",
    "        docs.append(text)\n",
    "        y.append(label)\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='log', random_state=1, max_iter=1)\n",
    "doc_stream = stream_docs(path='shuffled_movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.867\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define features based on word embeddings (pre-trained word2vec / Glove/Fastext emebddings can be used)\n",
    "- Define suitable d dimension, and sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing \"review\" to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i waited long to watch this movie. also because i like bruce willis. the plot was quite different from what i had expected but still quite good. its a good mix of emotions, humor and drama.<br /><br />left me thinking over and again :)'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['review'] = data['review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "data.at[49999,'review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting rid of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridof_sc(text):\n",
    "    text = re.sub('<[^>]*>', '', text) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i waited long to watch this movie. also because i like bruce willis. the plot was quite different from what i had expected but still quite good. its a good mix of emotions, humor and drama.left me thinking over and again :)'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['review'] = data['review'].apply(lambda x: ridof_sc(x))\n",
    "data.at[49999,'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2words(text, remove_stop = False):  \n",
    "    text = re.sub('[!)(#?,.:*\";]', ' ', text)\n",
    "    words = text.split()\n",
    "    if remove_stop:\n",
    "        stops = set(stopwords.words('english'))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #print(words)\n",
    "    return (words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function splits a text into sentences\n",
    "def review_sentences(review, remove_stopwords=False):\n",
    "    raw_sentences = sent_tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence)>0:\n",
    "            sentences.append(text2words(raw_sentence,remove_stopwords))\n",
    "\n",
    "    # sentences is a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the vocabulary for our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for review in data['review']:\n",
    "    sentences += review_sentences(review)\n",
    "#len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300  # Word vector dimensionality\n",
    "min_word_count = 30 # Minimum word count\n",
    "num_workers = 4     # Number of parallel threads\n",
    "context = 10        # Context window size\n",
    "downsampling = 1e-3 # (0.001) Downsample setting for frequent words\n",
    "\n",
    "# Initializing the train model\n",
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(sentences,\\\n",
    "                          workers=num_workers,\\\n",
    "                          size=num_features,\\\n",
    "                          min_count=min_word_count,\\\n",
    "                          window=context,\n",
    "                          sample=downsampling)\n",
    "\n",
    "# To make the model memory efficient\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# Saving the model for later use. Can be loaded using Word2Vec.load()\n",
    "model_name = \"300features_50minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.8535209894180298),\n",
       " ('flick', 0.6908314228057861),\n",
       " ('movies', 0.5725680589675903),\n",
       " ('it', 0.5693177580833435),\n",
       " ('picture', 0.515169620513916),\n",
       " ('documentary', 0.5139075517654419),\n",
       " ('show', 0.4913640022277832),\n",
       " ('sequel', 0.49041104316711426),\n",
       " ('turkey', 0.4816403388977051),\n",
       " ('episode', 0.45707225799560547)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test model\n",
    "model.wv.most_similar('movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.12615186 -0.08537848 -0.03888972  0.03662663 -0.02219864  0.00614061\n",
      " -0.12951656  0.05743549  0.00446866  0.03129537 -0.01344721 -0.01058722\n",
      "  0.10094015  0.09555555  0.03855471 -0.00639918 -0.0470705  -0.08892679\n",
      "  0.11756346 -0.02543444  0.09074035 -0.05586453  0.07222655  0.10098025\n",
      " -0.01262713  0.12632525  0.05631389  0.09006497  0.07177544  0.09999133\n",
      "  0.07672758  0.02365885 -0.0057018   0.08834656  0.00799534  0.06121251\n",
      "  0.01004557  0.06768848 -0.14478073  0.03759738  0.0674349   0.05043565\n",
      "  0.00957144 -0.04228305 -0.07052817 -0.1203943   0.07027616  0.05103814\n",
      " -0.00483615  0.00354183  0.01838845  0.01985047 -0.15733038 -0.08981951\n",
      "  0.05898672 -0.01374556 -0.04111585 -0.02021777  0.08237603 -0.00929134\n",
      "  0.12484758 -0.09824619  0.05074212 -0.03846441  0.04531216 -0.11685055\n",
      " -0.00900909 -0.04001853  0.04014616  0.13427156  0.10882665  0.0567045\n",
      "  0.0664423   0.03988168  0.06460241  0.06295266 -0.17584759 -0.08051222\n",
      "  0.1160237   0.1505221   0.01571569 -0.0245156   0.1041332   0.01419193\n",
      " -0.12073798 -0.06278167  0.13345648 -0.08519015 -0.0458511  -0.15196578\n",
      "  0.00311039  0.038728   -0.11311786 -0.10966007  0.19753462  0.01235765\n",
      "  0.01058858 -0.07851039 -0.02966398 -0.09173553 -0.04322451  0.00355346\n",
      " -0.01066655 -0.09597323 -0.02014625  0.10765071 -0.17482209  0.07924914\n",
      " -0.17594652 -0.11855553 -0.03115098  0.01339282  0.00056712  0.02593446\n",
      " -0.05160533  0.06959023  0.00243287  0.14746138  0.09844458  0.09722817\n",
      "  0.15920749 -0.03190234 -0.0304344   0.07110083  0.02104878  0.09004758\n",
      "  0.06051955 -0.06466098  0.05617443 -0.00347765  0.10866253  0.0816255\n",
      "  0.14134176  0.0624724  -0.1740855  -0.03343909 -0.04742194  0.02557386\n",
      "  0.03430263  0.1393632   0.123978   -0.00188698 -0.12446878  0.04988343\n",
      " -0.00263698  0.10707495  0.00525438  0.00073687  0.08543133 -0.15002286]\n"
     ]
    }
   ],
   "source": [
    "movie_vec = model.wv['movie']\n",
    "print(movie_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting avergae vector for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords += 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, num_features)\n",
    "        counter += 1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sol/.local/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 50000\n",
      "Review 2000 of 50000\n",
      "Review 3000 of 50000\n",
      "Review 4000 of 50000\n",
      "Review 5000 of 50000\n",
      "Review 6000 of 50000\n",
      "Review 7000 of 50000\n",
      "Review 8000 of 50000\n",
      "Review 9000 of 50000\n",
      "Review 10000 of 50000\n",
      "Review 11000 of 50000\n",
      "Review 12000 of 50000\n",
      "Review 13000 of 50000\n",
      "Review 14000 of 50000\n",
      "Review 15000 of 50000\n",
      "Review 16000 of 50000\n",
      "Review 17000 of 50000\n",
      "Review 18000 of 50000\n",
      "Review 19000 of 50000\n",
      "Review 20000 of 50000\n",
      "Review 21000 of 50000\n",
      "Review 22000 of 50000\n",
      "Review 23000 of 50000\n",
      "Review 24000 of 50000\n",
      "Review 25000 of 50000\n",
      "Review 26000 of 50000\n",
      "Review 27000 of 50000\n",
      "Review 28000 of 50000\n",
      "Review 29000 of 50000\n",
      "Review 30000 of 50000\n",
      "Review 31000 of 50000\n",
      "Review 32000 of 50000\n",
      "Review 33000 of 50000\n",
      "Review 34000 of 50000\n",
      "Review 35000 of 50000\n",
      "Review 36000 of 50000\n",
      "Review 37000 of 50000\n",
      "Review 38000 of 50000\n",
      "Review 39000 of 50000\n",
      "Review 40000 of 50000\n",
      "Review 41000 of 50000\n",
      "Review 42000 of 50000\n",
      "Review 43000 of 50000\n",
      "Review 44000 of 50000\n",
      "Review 45000 of 50000\n",
      "Review 46000 of 50000\n",
      "Review 47000 of 50000\n",
      "Review 48000 of 50000\n",
      "Review 49000 of 50000\n"
     ]
    }
   ],
   "source": [
    "reviews = []\n",
    "#for x in range(49997, 50000):\n",
    "for review in data['review']:\n",
    "    # to get the vectors, we remove stopwords\n",
    "    reviews.append(text2words(review, remove_stop=True))\n",
    "    \n",
    "DataVecs = getAvgFeatureVecs(reviews, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 300)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataVecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 50000)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = DataVecs.T\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50000)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data[['sentiment']]\n",
    "y = y.T\n",
    "y = y.values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X), type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(D, K):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    W -- initialized vector of shape (D, K)\n",
    "    b -- initialized scalar (corresponds to the bias) of size K\n",
    "    \"\"\"\n",
    "    \n",
    "    W = np.zeros((D,K))\n",
    "    b = np.zeros((K,1))\n",
    "    \n",
    "    return W, b\n",
    "\n",
    "def initialize_randomly(D, K):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    W -- initialized vector of shape (D, K)\n",
    "    b -- initialized scalar (corresponds to the bias) of size K\n",
    "    \"\"\"\n",
    "    \n",
    "    W = np.random.randn(D,K)*0.01\n",
    "    b = np.random.randn(K,1)*0.01\n",
    "    \n",
    "    return W, b\n",
    "\n",
    "def initialize_he(D, K):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    W -- initialized vector of shape (D, K)\n",
    "    b -- initialized scalar (corresponds to the bias) of size K\n",
    "    \"\"\"\n",
    "    \n",
    "    W = np.random.randn(D,K)*np.sqrt(2/D)\n",
    "    b = np.zeros((K,1))\n",
    "        \n",
    "    return W, b\n",
    "\n",
    "def initialize_params(D, K, init_type='zeros'):\n",
    "    if init_type == 'zeros':\n",
    "        print('zero-based init')\n",
    "        return initialize_with_zeros(D, K)\n",
    "    elif init_type == 'random':\n",
    "        print('random-based init')\n",
    "        return initialize_randomly(D, K)\n",
    "    elif init_type == 'he':\n",
    "        print('he-based init')\n",
    "        return initialize_he(D, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    \n",
    "    s = 1/(1+np.exp(-z))    \n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Y, Y_hat):\n",
    "           \n",
    "    m = Y.shape[1]\n",
    "    L = -(np.multiply(Y,np.log(Y_hat))+np.multiply((1-Y),np.log(1-Y_hat)))\n",
    "    L = (1/m)*np.sum(L)\n",
    "        \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(W, b, X, y, use_reg=False, reg_lambda=0.01):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dW -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    #print(\"Propagate\")\n",
    "    #print(\"W, b, X, y\")\n",
    "    #print(W.shape, b.shape, X.shape, y.shape)\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    z = np.dot(W.T,X) + b  \n",
    "    \n",
    "    #print(\"m, z = W.T * X + b\")\n",
    "    #print(m, z.shape)\n",
    "    \n",
    "    a = sigmoid(z)\n",
    "    \n",
    "    #print(\"a\")\n",
    "    #print(a.shape)\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    cost = compute_cost(y, a)\n",
    "    \n",
    "    #print(\"cost\")\n",
    "    #print(cost)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    if use_reg:\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)                \n",
    "        cost = cost + ((reg_lambda/2)*(np.sum(np.power(W,2))))        \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    dz = a - y\n",
    "    \n",
    "    #print(\"dz = a - y\")\n",
    "    #print(dz.shape)\n",
    "    \n",
    "    dW = (1/m)* np.dot(X,dz.T)\n",
    "    db = (1/m)* np.sum(dz)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    if use_reg:\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        #dW = dW + ((reg_lambda/2)*(np.sum(np.power(dW,2))))\n",
    "        dW += reg_lambda * W\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dW\": dW,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(W, b, X, y, num_iterations, learning_rate, use_reg = False, reg_lambda = 0.01, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    use_reg -- use regularization\n",
    "    reg_lambda -- regularization weight\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for ii in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        if use_reg:\n",
    "            #print(\"Using Reg\")\n",
    "            grads, cost = propagate(W, b, X, y,True, reg_lambda)\n",
    "        else:\n",
    "            grads, cost = propagate(W, b, X, y,False)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dW = grads[\"dW\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        #print(\"dW, db\")\n",
    "        #print(dW.shape, db.shape)\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        W = W - learning_rate*dW\n",
    "        b = b - learning_rate*db \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "            \n",
    "        # Record the costs\n",
    "        if ii % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "        # Print the cost every 200 training iterations\n",
    "        if print_cost and ii % 200 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(ii, cost))\n",
    "    \n",
    "    params = {\"W\": W,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dW\": dW,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    z = np.dot(W.T,X) + b    \n",
    "    A = sigmoid(z)\n",
    "    \n",
    "    #print(\"Predict\")\n",
    "    #print(\"A\")\n",
    "    #print(A.shape)\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    for ii in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        if A[0,ii] >= 0.5:\n",
    "            Y_prediction[0,ii] = 1\n",
    "        else:\n",
    "            Y_prediction[0,ii] = 0\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, D=2,K=2, num_iterations=2000, learning_rate=0.5, use_reg=True, reg_lambda=0.01, init_type='zeros', print_cost=False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    W,b = initialize_params(D, K, init_type)\n",
    "    #print(W.shape)\n",
    "    #print(b.shape)\n",
    "    #print(W)\n",
    "    #print(b)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    #print(use_reg)\n",
    "    parameters, grads, costs = optimize(W, b, X_train, y_train, num_iterations, learning_rate,use_reg, reg_lambda,print_cost)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    W = parameters[\"W\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    Y_prediction_train = predict(W, b, X_train)\n",
    "    \n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - y_train)) * 100))\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"W\" : W, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he-based init\n",
      "Cost after iteration 0: 1.387265\n",
      "Cost after iteration 200: 1.296872\n",
      "Cost after iteration 400: 1.233559\n",
      "Cost after iteration 600: 1.186051\n",
      "Cost after iteration 800: 1.148387\n",
      "Cost after iteration 1000: 1.117296\n",
      "Cost after iteration 1200: 1.090877\n",
      "Cost after iteration 1400: 1.067954\n",
      "Cost after iteration 1600: 1.047754\n",
      "Cost after iteration 1800: 1.029741\n",
      "Cost after iteration 2000: 1.013528\n",
      "Cost after iteration 2200: 0.998822\n",
      "Cost after iteration 2400: 0.985399\n",
      "Cost after iteration 2600: 0.973079\n",
      "Cost after iteration 2800: 0.961717\n",
      "train accuracy: 80.684 %\n"
     ]
    }
   ],
   "source": [
    "# train model here (≈ 1 line of code), e.g. with learning_rate = 0.05\n",
    "### START CODE HERE ### (≈ 1 line of code)\n",
    "d =  train(X, y,num_features, 2, 3000, 0.8, False, 0.01, 'he', True)\n",
    "### END CODE HERE ###\n",
    "W = d['W']\n",
    "b = d['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he-based init\n",
      "Cost after iteration 0: 1.387855\n",
      "Cost after iteration 200: 1.297979\n"
     ]
    }
   ],
   "source": [
    "# train model here (≈ 1 line of code), e.g. with learning_rate = 0.05\n",
    "### START CODE HERE ### (≈ 1 line of code)\n",
    "d =  train(X, y,num_features, 2, 4000, 0.8, False, 0.01, 'he', True)\n",
    "### END CODE HERE ###\n",
    "W = d['W']\n",
    "b = d['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 50000) (1, 50000)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 300) (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "XX = X.T\n",
    "yy = y.T\n",
    "print(XX.shape,yy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 45000)\n",
      "(300, 5000)\n",
      "(1, 45000)\n",
      "(1, 5000)\n"
     ]
    }
   ],
   "source": [
    "X_train_, X_valid_, y_train_, y_valid_ = XX[:45000], XX[:5000], yy[:45000], yy[45000:]\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 45000)\n",
      "(300, 5000)\n",
      "(1, 45000)\n",
      "(1, 5000)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_.T\n",
    "X_valid = X_valid_.T\n",
    "y_train = y_train_.T\n",
    "y_valid = y_valid_.T\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://nbviewer.jupyter.org/github/rasbt/pattern_classification/blob/master/machine_learning/scikit-learn/outofcore_modelpersistence.ipynb\n",
    "\n",
    "https://taylorwhitten.github.io/blog/word2vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
